import { readFile, writeFile } from 'node:fs/promises'
import path from 'node:path'
import pdfParser from 'pdf-parse'
import ollama from 'ollama'
import { QdrantClient } from '@qdrant/js-client-rest'
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { StringOutputParser } from '@langchain/core/output_parsers'
import { ChatGroq } from '@langchain/groq'

interface Point {
  id: string | number
  version: number
  score: number
  payload?:
    | Record<string, unknown>
    | {
        [key: string]: unknown
      }
    | null
    | undefined
  vector?:
    | Record<string, unknown>
    | number[]
    | number[][]
    | {
        [key: string]:
          | number[]
          | number[][]
          | {
              text: string
              model?: string | null | undefined
            }
          | {
              indices: number[]
              values: number[]
            }
          | undefined
      }
    | {
        text: string
        model?: string | null | undefined
      }
    | null
    | undefined
  shard_key?: string | number | Record<string, unknown> | null | undefined
  order_value?: number | Record<string, unknown> | null | undefined
}

/* 
  set this to true to create a new collection
  in the vector db and populate it with vector data
  from the book
*/
const initializeCollection = true
const documentName = 'resumo-de-cg'

const GROQ_API_KEY = process.env.GROQ_API_KEY

const qdrant = new QdrantClient({
  host: 'localhost',
  port: 6333
})

/**
 * Function to split text into chunks
 */
const chunkText = (text: string, maxLength = 1000) => {
  const sentences = text.split(/(?<=[.?!])\s+/) // split by sentences
  const chunks = []
  let chunk = ''

  sentences.forEach(sentence => {
    if ((chunk + sentence).length > maxLength) {
      chunks.push(chunk.trim())
      chunk = ''
    }
    chunk += sentence + ' '
  })

  if (chunk.trim()) {
    chunks.push(chunk.trim())
  }

  return chunks
}

/**
 * Function to merge chunks into a single coherent context
 */
function mergeChunks(points: Point[]) {
  const uniqueChunks = new Set() // prevent duplications

  points.forEach(point => {
    const content = point.payload?.content || ''
    uniqueChunks.add((content as string).trim())
  })

  return Array.from(uniqueChunks).join('\n\n')
}
async function main() {
  if (initializeCollection) {
    const __dirname = import.meta.dirname

    const pdfFile = await readFile(
      path.join(__dirname, 'files', `${documentName}.pdf`)
    )

    const { text } = await pdfParser(pdfFile)

    await writeFile(
      path.join(__dirname, 'files', `${documentName}.txt`),
      text.trim()
    )

    const textChunks = chunkText(text.trim(), 1000)

    const collectionAlreadyExists = await qdrant.collectionExists(documentName)

    if (collectionAlreadyExists.exists) {
      console.log('üî• Deleting collection with the same name...\n')
      const collectionDeleted = await qdrant.deleteCollection(documentName)

      if (!collectionDeleted) {
        console.log(
          `üìç Something went wrong, we couldn't delete the collection ${documentName}\n`
        )
        return
      }

      console.log(`üéâ Collection ${documentName} deleted succesfully\n`)
    }

    const collectionCreadted = await qdrant.createCollection(documentName, {
      vectors: {
        size: 1024,
        distance: 'Cosine'
      }
    })

    if (!collectionCreadted) {
      console.log(
        `üìç Something went wrong, we couldn't create the collection ${documentName}\n`
      )
      return
    }

    console.log(`üéâ Collection ${documentName} created succesfully\n`)

    for (let i = 0; i < textChunks.length; i++) {
      const chunk = textChunks[i]

      const { embedding } = await ollama.embeddings({
        model: 'mxbai-embed-large',
        prompt: chunk
      })

      await qdrant.upsert(documentName, {
        points: [
          {
            id: i + 1,
            vector: embedding,
            payload: {
              content: chunk
            }
          }
        ]
      })
    }

    console.log(`‚ú® Collection ${documentName} is set and ready`)
    return
  }

  const llm = new ChatGroq({
    model: 'mixtral-8x7b-32768',
    temperature: 1,
    maxRetries: undefined,
    streaming: true,
    maxTokens: undefined,
    apiKey: GROQ_API_KEY
  })

  const question = 'Qual √© a li√ß√£o que o pequeno pr√≠ncipe aprende com a raposa?'

  // generate the question as an embedding
  const { embedding } = await ollama.embeddings({
    model: 'mxbai-embed-large',
    prompt: `${question}`
  })

  // get relevant points
  const { points } = await qdrant.query(documentName, {
    query: embedding,
    with_payload: true,
    limit: 6
  })

  const content = mergeChunks(points)

  // console.log('Contexto retornado do Qdrant:\n', content)

  const prompt = ChatPromptTemplate.fromMessages([
    {
      role: 'system',
      content: `
          Voc√™ √© uma IA projetada para responder e conversar exclusivamente com base nas informa√ß√µes contidas no documento que eu fornecerei. Nenhuma informa√ß√£o externa pode ser utilizada, e voc√™ n√£o pode fazer suposi√ß√µes nem inventar respostas. Se a resposta n√£o estiver expl√≠cita no documento, voc√™ deve dizer: "Desculpe, essa informa√ß√£o n√£o est√° dispon√≠vel no documento" e nada mais.

          Voc√™ deve manter uma conversa natural, sendo educado e fluente, mas sem se desviar do conte√∫do. Se eu perguntar algo que n√£o est√° no documento, voc√™ n√£o deve buscar respostas externas. Sempre que poss√≠vel, forne√ßa respostas claras e objetivas, diretamente relacionadas ao conte√∫do do documento.

          **Importante:**
          - N√£o adicione nenhuma informa√ß√£o que n√£o esteja no documento.
          - Caso a informa√ß√£o que eu pedir n√£o esteja no documento, seja claro e direto: "Desculpe, essa informa√ß√£o n√£o est√° dispon√≠vel no documento."
          - O seu objetivo √© fornecer respostas √∫teis dentro dos limites do que est√° no conte√∫do do documento. N√£o se preocupe em fornecer mais contexto ou explicar o que n√£o est√° no texto.
          - N√£o fa√ßa conjecturas ou suposi√ß√µes, mas se necess√°rio, d√™ um contexto que esteja no documento e que ajude a responder a pergunta mais claramente.
          - Seja amig√°vel e prest√°vel.
          - D√™ respostas completas que fa√ßam sentido a pergunta.

          Agora, voc√™ est√° pronto para come√ßar a conversa. Fique √† vontade para me perguntar qualquer coisa relacionada ao conte√∫do que est√° no documento.

          O documento √© este: {file}
        `
    },
    {
      role: 'user',
      content: `${question}`
    }
  ])

  const chain = prompt.pipe(llm).pipe(new StringOutputParser())

  const response = chain.streamEvents(
    {
      file: content
    },
    {
      version: 'v2'
    }
  )

  let chatResponse = ''

  for await (const stream of response) {
    if (stream.event === 'on_chat_model_stream') {
      process.stdout.write(stream.data.chunk.content)

      chatResponse += stream.data.chunk.content
    }
  }

  console.log('\n')
}

main()
